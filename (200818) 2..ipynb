{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>■ 딥러닝 복습</b>\n",
    "    1장. numpy\n",
    "    2장. 퍼셉트론\n",
    "    3장. 3층 신경망 구현\n",
    "    4장. 2층 신경망 구현(수치미분)\n",
    "    5장. 2층 신경망 구현(오차역전파)      tensorflow 1.x -> tensorflow 2.x\n",
    "    --------------------------------------------------------------------\n",
    "    6장. 신경망 학습시키는 기술들\n",
    "    7장. CNN을 이용한 신경망 구현\n",
    "    -------------------------------------------------------------------- 자전거 타는 법\n",
    "\n",
    "    6장. 신경망 학습시키는 기술들\n",
    "        1. 언더피팅 방지하는 방법\n",
    "            - 가중치 초기값 선정\n",
    "                ① Xavier\n",
    "                ② He\n",
    "            - 배치 정규화\n",
    "        2. 오버피팅 방지하는 방법\n",
    "### <b>■ 텐서플로우로 가중치 초기값 선정하는 방법</b>\n",
    "    1. Xavier\n",
    "![fig6-14](fig6-13.png)\n",
    "\n",
    "$$ {{1} \\over {\\sqrt{n}}} \\cdot \\rm{np.random.randn(r,c)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T01:04:03.960180Z",
     "start_time": "2020-08-18T01:03:38.311345Z"
    }
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_variable(name='W1', shape=[784,50], initializer = tf.contrib.layers.xavier_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. He 가중치 초기값 구성\n",
    "$$ \\sqrt{{{2} \\over {n}}} \\cdot \\rm{np.random.randn(r,c)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_variable(name=\"W1\", shape=[784,50], initializer = tf.contrib.layers.variance_scaling_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제1. 어제 마지막 문제로 만들었던 텐서 플로우로 구현한 신경망 코드에 가중치 초기값을 xavier로 해서 구현하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T01:30:16.839684Z",
     "start_time": "2020-08-18T01:30:01.454704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-a5d4a56c599c>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\knitwill\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\knitwill\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\knitwill\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\knitwill\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\knitwill\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "{1} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.92\n",
      "{2} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.98\n",
      "{3} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.99\n",
      "{4} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.94\n",
      "{5} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.97\n",
      "{6} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "{7} 에폭 훈련데이터 정확도 :  0.96 \t 테스트 데이터 정확도: 0.97\n",
      "{8} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.95\n",
      "{9} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.96\n",
      "{10} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.98\n",
      "{11} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.95\n",
      "{12} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.96\n",
      "{13} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.97\n",
      "{14} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.95\n",
      "{15} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.94\n",
      "{16} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.96\n",
      "{17} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "{18} 에폭 훈련데이터 정확도 :  0.97 \t 테스트 데이터 정확도: 0.97\n",
      "{19} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "{20} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# 은닉1층\n",
    "x = tf.placeholder('float',[None,784])\n",
    "W1 = tf.get_variable(name='W1', shape=[784,50], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.ones([1,50]))\n",
    "\n",
    "y = tf.matmul(x, W1) + b1\n",
    "y_hat = tf.nn.relu(y)\n",
    "\n",
    "# 출력층\n",
    "W2 = tf.get_variable(name='W2', shape=[50,10], initializer = tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.ones([1,10]))\n",
    "\n",
    "z = tf.matmul(y_hat,W2) + b2\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder('float',[None,10]) # 정답 데이터를 담을 배열\n",
    "y_label = tf.argmax(y_onehot, axis=1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction,'float'))\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.0000001), axis=1)\n",
    "rs = tf.reduce_mean(loss)\n",
    "\n",
    "# 학습\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 그래프 실행\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1,601*20):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    batch_x_test, batch_y_test = mnist.test.next_batch(100)\n",
    "    sess.run(train, feed_dict={x:batch_xs, y_onehot:batch_ys})\n",
    "    if not i % 600:\n",
    "        print(i//600,\"에폭 훈련데이터 정확도 : \",sess.run(accuracy, feed_dict={x:batch_xs, y_onehot:batch_ys}),\"\\t\",\"테스트 데이터 정확도:\", sess.run(accuracy, feed_dict={x:batch_x_test, y_onehot:batch_y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제137. 이번에는 가중치 초기값을 He로 해서 수행하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T01:30:33.997474Z",
     "start_time": "2020-08-18T01:30:18.283304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "{1} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.96\n",
      "{2} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 1.0\n",
      "{3} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "{4} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.99\n",
      "{5} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "{6} 에폭 훈련데이터 정확도 :  0.97 \t 테스트 데이터 정확도: 0.93\n",
      "{7} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.98\n",
      "{8} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.97\n",
      "{9} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.97\n",
      "{10} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.98\n",
      "{11} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "{12} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "{13} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.95\n",
      "{14} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "{15} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "{16} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.96\n",
      "{17} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "{18} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "{19} 에폭 훈련데이터 정확도 :  0.97 \t 테스트 데이터 정확도: 0.97\n",
      "{20} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.96\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "tf.reset_default_graph() # 텐서 그래프 초기화 하는 코드\n",
    "\n",
    "# 은닉1층\n",
    "x = tf.placeholder('float',[None,784])\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[784,50], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b1 = tf.Variable(tf.ones([1,50]))\n",
    "\n",
    "y = tf.matmul(x, W1) + b1\n",
    "y_hat = tf.nn.relu(y)\n",
    "\n",
    "# 출력층\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[50,10], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.Variable(tf.ones([1,10]))\n",
    "\n",
    "z = tf.matmul(y_hat,W2) + b2\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder('float',[None,10]) # 정답 데이터를 담을 배열\n",
    "y_label = tf.argmax(y_onehot, axis=1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction,'float'))\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.0000001), axis=1)\n",
    "rs = tf.reduce_mean(loss)\n",
    "\n",
    "# 학습\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 그래프 실행\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1,601*20):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    batch_x_test, batch_y_test = mnist.test.next_batch(100)\n",
    "    sess.run(train, feed_dict={x:batch_xs, y_onehot:batch_ys})\n",
    "    if not i % 600:\n",
    "        print(i//600,\"에폭 훈련데이터 정확도 : \",sess.run(accuracy, feed_dict={x:batch_xs, y_onehot:batch_ys}),\"\\t\",\"테스트 데이터 정확도:\", sess.run(accuracy, feed_dict={x:batch_x_test, y_onehot:batch_y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제138. 위의 2층 신경망을 3층 신경망으로 변경하시오\n",
    "    기존층 : 입력층 -------> 은닉1층 ------> 출력층\n",
    "             784             100             10\n",
    "    변경후 : 입력층 -------> 은닉1층 ------> 은닉2층 -------> 출력층\n",
    "             784             100             50              10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T01:31:26.072349Z",
     "start_time": "2020-08-18T01:31:05.755894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "{1} 에폭 훈련데이터 정확도 :  0.91 \t 테스트 데이터 정확도: 0.95\n",
      "{2} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.97\n",
      "{3} 에폭 훈련데이터 정확도 :  0.97 \t 테스트 데이터 정확도: 0.98\n",
      "{4} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.93\n",
      "{5} 에폭 훈련데이터 정확도 :  0.97 \t 테스트 데이터 정확도: 0.97\n",
      "{6} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.96\n",
      "{7} 에폭 훈련데이터 정확도 :  0.97 \t 테스트 데이터 정확도: 0.96\n",
      "{8} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.96\n",
      "{9} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.96\n",
      "{10} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.97\n",
      "{11} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.97\n",
      "{12} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "{13} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "{14} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "{15} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.94\n",
      "{16} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.93\n",
      "{17} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.98\n",
      "{18} 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.98\n",
      "{19} 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.96\n",
      "{20} 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "tf.reset_default_graph() # 텐서 그래프 초기화 하는 코드\n",
    "\n",
    "# 은닉1층\n",
    "x = tf.placeholder('float',[None,784])\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[784,100], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b1 = tf.Variable(tf.ones([1,100]))\n",
    "\n",
    "y = tf.matmul(x, W1) + b1\n",
    "y_hat = tf.nn.relu(y)\n",
    "\n",
    "# 은닉2층\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[100,50], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.Variable(tf.ones([1,50]))\n",
    "\n",
    "y2 = tf.matmul(y_hat,W2) + b2\n",
    "y2_hat = tf.nn.relu(y2)\n",
    "\n",
    "# 출력층\n",
    "W3 = tf.get_variable(name=\"W3\", shape=[50,10], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b3 = tf.Variable(tf.ones([1,10]))\n",
    "\n",
    "z = tf.matmul(y2_hat,W3) + b3\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder('float',[None,10]) # 정답 데이터를 담을 배열\n",
    "y_label = tf.argmax(y_onehot, axis=1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction,'float'))\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.0000001), axis=1)\n",
    "rs = tf.reduce_mean(loss)\n",
    "\n",
    "# 학습\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 그래프 실행\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1,601*20):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    batch_x_test, batch_y_test = mnist.test.next_batch(100)\n",
    "    sess.run(train, feed_dict={x:batch_xs, y_onehot:batch_ys})\n",
    "    if not i % 600:\n",
    "        print(i//600,\"에폭 훈련데이터 정확도 : \",sess.run(accuracy, feed_dict={x:batch_xs, y_onehot:batch_ys}),\"\\t\",\"테스트 데이터 정확도:\", sess.run(accuracy, feed_dict={x:batch_x_test, y_onehot:batch_y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>■ 텐서 플로우로 배치 정규화 구현하는 방법</b>\n",
    "    배치 정규화 - 신경망 학습시 가중치 값의 데이터가 골고루 분산될 수 있도록 강제화 하는 장치\n",
    "        층이 깊어져도 가중치의 정규분포를 계속 유지할 수 있도록 층마다 강제화 하는 장치\n",
    "    \n",
    "```python\n",
    "batch_z1 = tf.contrib.layers.batch_norm(z1, True)\n",
    "```\n",
    "    Affine1 ------> 배치 정규화 ------> ReLU\n",
    "     (z1)\n",
    "     \n",
    "#### 예제1. 지금까지 완성한 신경망에 배치 정규화를 은닉 1층에 구현하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T02:07:43.268902Z",
     "start_time": "2020-08-18T02:07:01.848674Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "1 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.97\n",
      "2 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "3 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.98\n",
      "4 에폭 훈련데이터 정확도 :  0.96 \t 테스트 데이터 정확도: 0.98\n",
      "5 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "6 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "7 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.94\n",
      "8 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.94\n",
      "9 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "10 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "11 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.98\n",
      "12 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "13 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.93\n",
      "14 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "15 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.95\n",
      "16 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 1.0\n",
      "17 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "18 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "19 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 1.0\n",
      "20 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "tf.reset_default_graph() # 텐서 그래프 초기화 하는 코드\n",
    "\n",
    "# 은닉1층\n",
    "x = tf.placeholder('float',[None,784])\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[784,100], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b1 = tf.Variable(tf.ones([1,100]))\n",
    "\n",
    "y1 = tf.matmul(x, W1) + b1\n",
    "\n",
    "batch_y1 = tf.contrib.layers.batch_norm(y1, True)\n",
    "\n",
    "y1_hat = tf.nn.relu(batch_y1)\n",
    "\n",
    "# 은닉2층\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[100,50], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.Variable(tf.ones([1,50]))\n",
    "\n",
    "y2 = tf.matmul(y1_hat,W2) + b2\n",
    "y2_hat = tf.nn.relu(y2)\n",
    "\n",
    "# 출력층\n",
    "W3 = tf.get_variable(name=\"W3\", shape=[50,10], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b3 = tf.Variable(tf.ones([1,10]))\n",
    "\n",
    "z = tf.matmul(y2_hat,W3) + b3\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder('float',[None,10]) # 정답 데이터를 담을 배열\n",
    "y_label = tf.argmax(y_onehot, axis=1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction,'float'))\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.0000001), axis=1)\n",
    "rs = tf.reduce_mean(loss)\n",
    "\n",
    "# 학습\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 그래프 실행\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1,601*20):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    batch_x_test, batch_y_test = mnist.test.next_batch(100)\n",
    "    sess.run(train, feed_dict={x:batch_xs, y_onehot:batch_ys})\n",
    "    if not i % 600:\n",
    "        print(i//600,\"에폭 훈련데이터 정확도 : \",sess.run(accuracy, feed_dict={x:batch_xs, y_onehot:batch_ys}),\"\\t\",\"테스트 데이터 정확도:\", sess.run(accuracy, feed_dict={x:batch_x_test, y_onehot:batch_y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※ 문제139. 은닉 2층에도 배치 정규화를 적용하시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T02:10:48.502770Z",
     "start_time": "2020-08-18T02:09:57.254672Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "1 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.99\n",
      "2 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.96\n",
      "3 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.95\n",
      "4 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "5 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "6 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "7 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.97\n",
      "8 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "9 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "10 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.97\n",
      "11 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 1.0\n",
      "12 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "13 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "14 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "15 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 1.0\n",
      "16 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "17 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "18 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "19 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "20 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "tf.reset_default_graph() # 텐서 그래프 초기화 하는 코드\n",
    "\n",
    "# 은닉1층\n",
    "x = tf.placeholder('float',[None,784])\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[784,100], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b1 = tf.Variable(tf.ones([1,100]))\n",
    "\n",
    "y1 = tf.matmul(x, W1) + b1\n",
    "\n",
    "batch_y1 = tf.contrib.layers.batch_norm(y1, True)\n",
    "\n",
    "y1_hat = tf.nn.relu(batch_y1)\n",
    "\n",
    "# 은닉2층\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[100,50], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.Variable(tf.ones([1,50]))\n",
    "\n",
    "y2 = tf.matmul(y1_hat,W2) + b2\n",
    "\n",
    "batch_y2 = tf.contrib.layers.batch_norm(y2, True)\n",
    "\n",
    "y2_hat = tf.nn.relu(batch_y2)\n",
    "\n",
    "# 출력층\n",
    "W3 = tf.get_variable(name=\"W3\", shape=[50,10], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b3 = tf.Variable(tf.ones([1,10]))\n",
    "\n",
    "z = tf.matmul(y2_hat,W3) + b3\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder('float',[None,10]) # 정답 데이터를 담을 배열\n",
    "y_label = tf.argmax(y_onehot, axis=1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction,'float'))\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.0000001), axis=1)\n",
    "rs = tf.reduce_mean(loss)\n",
    "\n",
    "# 학습\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 그래프 실행\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1,601*20):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    batch_x_test, batch_y_test = mnist.test.next_batch(100)\n",
    "    sess.run(train, feed_dict={x:batch_xs, y_onehot:batch_ys})\n",
    "    if not i % 600:\n",
    "        print(i//600,\"에폭 훈련데이터 정확도 : \",sess.run(accuracy, feed_dict={x:batch_xs, y_onehot:batch_ys}),\"\\t\",\"테스트 데이터 정확도:\", sess.run(accuracy, feed_dict={x:batch_x_test, y_onehot:batch_y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T02:10:48.502770Z",
     "start_time": "2020-08-18T02:09:57.254672Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "1 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.99\n",
      "2 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 0.96\n",
      "3 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.95\n",
      "4 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "5 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "6 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "7 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.97\n",
      "8 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "9 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.99\n",
      "10 에폭 훈련데이터 정확도 :  0.98 \t 테스트 데이터 정확도: 0.97\n",
      "11 에폭 훈련데이터 정확도 :  0.99 \t 테스트 데이터 정확도: 1.0\n",
      "12 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "13 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "14 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "15 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 1.0\n",
      "16 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.98\n",
      "17 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "18 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "19 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n",
      "20 에폭 훈련데이터 정확도 :  1.0 \t 테스트 데이터 정확도: 0.97\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "\n",
    "tf.reset_default_graph() # 텐서 그래프 초기화 하는 코드\n",
    "\n",
    "# 은닉1층\n",
    "x = tf.placeholder('float',[None,784])\n",
    "x1 = tf.reshape(x,[-1,28,28,1]) # 흑백사진, 1층, batch 개수를 모르므로 -1. 2차원 -> 4차원으로 변경\n",
    "\n",
    "# Convolution 1층\n",
    "W1 = tf.Variable(tf.random_normal)\n",
    "\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[784,100], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b1 = tf.Variable(tf.ones([1,100]))\n",
    "\n",
    "y1 = tf.matmul(x, W1) + b1\n",
    "\n",
    "batch_y1 = tf.contrib.layers.batch_norm(y1, True)\n",
    "\n",
    "y1_hat = tf.nn.relu(batch_y1)\n",
    "\n",
    "# 은닉2층\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[100,50], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b2 = tf.Variable(tf.ones([1,50]))\n",
    "\n",
    "y2 = tf.matmul(y1_hat,W2) + b2\n",
    "\n",
    "batch_y2 = tf.contrib.layers.batch_norm(y2, True)\n",
    "\n",
    "y2_hat = tf.nn.relu(batch_y2)\n",
    "\n",
    "# 출력층\n",
    "W3 = tf.get_variable(name=\"W3\", shape=[50,10], initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "b3 = tf.Variable(tf.ones([1,10]))\n",
    "\n",
    "z = tf.matmul(y2_hat,W3) + b3\n",
    "z_hat = tf.nn.softmax(z)\n",
    "y_predict = tf.argmax(z_hat, axis=1)\n",
    "\n",
    "# 정확도 확인\n",
    "y_onehot = tf.placeholder('float',[None,10]) # 정답 데이터를 담을 배열\n",
    "y_label = tf.argmax(y_onehot, axis=1)\n",
    "\n",
    "correction_prediction = tf.equal(y_predict, y_label)\n",
    "accuracy = tf.reduce_mean(tf.cast(correction_prediction,'float'))\n",
    "\n",
    "# 오차 확인\n",
    "loss = -tf.reduce_sum(y_onehot * tf.log(z_hat+0.0000001), axis=1)\n",
    "rs = tf.reduce_mean(loss)\n",
    "\n",
    "# 학습\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 변수 초기화\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 그래프 실행\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(1,601*20):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    batch_x_test, batch_y_test = mnist.test.next_batch(100)\n",
    "    sess.run(train, feed_dict={x:batch_xs, y_onehot:batch_ys})\n",
    "    if not i % 600:\n",
    "        print(i//600,\"에폭 훈련데이터 정확도 : \",sess.run(accuracy, feed_dict={x:batch_xs, y_onehot:batch_ys}),\"\\t\",\"테스트 데이터 정확도:\", sess.run(accuracy, feed_dict={x:batch_x_test, y_onehot:batch_y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
